{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6dec13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string \n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline \n",
    "## mathplot 그래프를 출력 영역에 표시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c7cfdb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zipfile.ZipFile(\"data.zip\").extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f8fcfbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어 stoi, itos화\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1:\"EOS\"}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "841727a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#유니 코드 문자열을 일반 ASCII로 변환\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "    c for c in unicodedata.normalize('NFD',s) \n",
    "    if unicodedata.category(c) != 'Mn' )\n",
    "\n",
    "##소문자로 만들기, 문자 아닌 문자 제거.\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip()) ## strip(), 양 끝의 공백을 지움\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) ##지우는게 아니라 한칸 띄어쓰기가 적용됨\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "82207436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readLang(lang1, lang2, reverse=False):\n",
    "    print(\"Read lines...\")\n",
    "    \n",
    "    ##파일을 읽고 줄로 분리\n",
    "    lines = open(f'data/{lang1}-{lang2}.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    ##모든 줄을 쌍으로 분리하고 정규화\n",
    "    pairs =[[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9beab",
   "metadata": {},
   "source": [
    "\n",
    "    lines = open(f'data/eng-fra.txt', encoding='utf-8').read()\n",
    "     = 'Go.\\tVa !\\nRun!\\tCours\\u202f!\\nRun!\\tCourez\\u202f!\\nWow!\\tÇa alors\\u202f'\n",
    "\n",
    "    lines = open(f'data/eng-fra.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "     = ['Go.\\tVa !',\n",
    "     'Run!\\tCours\\u202f!',\n",
    "     'Run!\\tCourez\\u202f!',\n",
    "     'Wow!\\tÇa alors\\u202f!',\n",
    "     'Fire!\\tAu feu !',\n",
    "     \"Help!\\tÀ l'aide\\u202f!\",\n",
    "     'Jump.\\tSaute.',\n",
    "     'Stop!\\tÇa suffit\\u202f!',\n",
    "     'Stop!\\tStop\\u202f!',\n",
    "     'Stop!\\tArrête-toi !']\n",
    "\n",
    "     pairs= [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "      = [['go .', 'va !'],\n",
    "     ['run !', 'cours !'],\n",
    "     ['run !', 'courez !'],\n",
    "     ['wow !', 'ca alors !'],\n",
    "     ['fire !', 'au feu !'],\n",
    "     ['help !', 'a l aide !'],\n",
    "     ['jump .', 'saute .'],\n",
    "     ['stop !', 'ca suffit !'],\n",
    "     ['stop !', 'stop !'],\n",
    "     ['stop !', 'arrete toi !']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ac538d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "db35be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes) ##eng_prefixes에만 있는 단어 True, False 반환, 결과적으로 True만 반환\n",
    "\n",
    "##10글자 미만 시작이 eng_prefixes로 시작하는 문장들만."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9594196b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10599"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4bb21f",
   "metadata": {},
   "source": [
    "    pairs[0][1].split(' ')\n",
    "     = ['va', '!']\n",
    "\n",
    "    print(pairs[9]) \n",
    "    a = filterPair(pairs[9])\n",
    "    print(a)\n",
    "    =['stop !', 'arrete toi !']\n",
    "     False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "63040312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631e817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ca295787",
   "metadata": {},
   "outputs": [],
   "source": [
    "##파일을 읽고 줄로 분리, 줄을 쌍으로 분리\n",
    "## 텍스트 정규화하고 길이,내용으로 필터링 , pre_fix filtering\n",
    "## 쌍을 이룬 문장들로 단어 리스트 생성\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLang(lang1, lang2, reverse)\n",
    "    \n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    #pairs = filterPairs(pairs)\n",
    "    \n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "8eb3de75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 135842 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 21334\n",
      "eng 13043\n",
      "['qu as tu d autre dans la poche ?', 'what else do you have in your pocket ?']\n"
     ]
    }
   ],
   "source": [
    "##lnput_lang 단어 사전\n",
    "input_lang, output_lang, pairs = prepareData('eng','fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "1e253025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size , hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size) ##hidden_size 크기를 받아서 hidden_size로 출력\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output = embedded ##(1, 1, hidden_size)\n",
    "        output, hidden = self.gru(output, hidden) #(1,1,hidden), (hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "2518eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        output = self.embedding(inputs).view(1,1,-1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        a = self.softmax(self.out(output[0]))\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size , device =device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "eb6608e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype = torch.long, device = device).view(-1,1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "7aa648ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1077/1219565929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlenFromPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1077/1219565929.py\u001b[0m in \u001b[0;36mlenFromPair\u001b[0;34m(pair)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def lenFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "for i in pairs:\n",
    "    c, b = 0\n",
    "    a = lenFromPair(i)\n",
    "    if a[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "3f08e2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs[135841][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "33ef29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "## batch_first = True (N, L, H ) // batch_first = False (L, N, H) \n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length = MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    #encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "        #encoder_outputs[ei] = encoder_output[0,0] ##0번째 시퀀스, 0번째 배치의, 히든 사이즈 attention 용\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device= device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        ## 교사학습이 이뤄지면 타겟단어를 다음 입력으로 전달\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) #, encoder_outputs\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di] #teacher forcing\n",
    "    \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)#, encoder_outputs\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1) ##각 배치의 각 시퀀스 가장 큰 수 하나를 뽑음, 그 값과 인덱스를 반환함. \n",
    "            decoder_input = topi.squeeze().detach() ##입력으로 사용할 부분을 히스토리에서 분리 그래디언트 계산 해제, 입력값 이기 때문에.\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "730f1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    \n",
    "    return f'{m}m {s}s'\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    \n",
    "    es = s / (percent)\n",
    "    \n",
    "    rs = es - s\n",
    "    \n",
    "    return f\"{asMinutes(s)} (- {asMinutes(rs)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "44c8ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every = 1000, plot_every = 100, learning_rate = 0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr = learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr = learning_rate)\n",
    "    \n",
    "    training_pairs = [tensorFromPair(random.choice(pairs)) for i in range(n_iters)] ## list 안에 (source, target) 형태로 생성\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters+1):\n",
    "        training_pair = training_pairs[iter-1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "    \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                     iter, iter / n_iters * 100, print_loss_avg))        \n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "        \n",
    "    showPlot(plot_losses)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "10caa63b",
   "metadata": {},
   "outputs": [],
   "source": [
    " training_pairs = [tensorFromPair(random.choice(pairs)) for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "7a2ec316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[138],\n",
       "          [119],\n",
       "          [604],\n",
       "          [ 69],\n",
       "          [198],\n",
       "          [  3],\n",
       "          [  1]], device='cuda:0'),\n",
       "  tensor([[ 188],\n",
       "          [ 189],\n",
       "          [ 357],\n",
       "          [1145],\n",
       "          [  94],\n",
       "          [2926],\n",
       "          [   3],\n",
       "          [   1]], device='cuda:0'))]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7c720b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base= 0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "f7148a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        #encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            #encoder_outputs[ei] = encoder_output[0,0]\n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device = device)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_word = []\n",
    "        \n",
    "        for di in range(max_length): ## 1,1, hidden 값이 들어감\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) #if attention, encoder_outputs 추가\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_word.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_word.append(output_lang.index2word[topi.item()])\n",
    "                \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            \n",
    "        \n",
    "        return decoded_word\n",
    "            \n",
    "                \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927243a",
   "metadata": {},
   "source": [
    "## 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "8068518c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1077/370835359.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1077/1786098103.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1077/2277202326.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "trainIters(encoder, decoder, 75000, print_every= 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "68c4899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6e35204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words = evaluate(encoder, decoder, pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "05b1e5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output_words))\n",
    "print(type(pair[1].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0a3a9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_BLEU(encoder, decoder):\n",
    "    total_bleu = 0\n",
    "    for i in range(len(pairs)):\n",
    "        pair = pairs[i]\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        real_words = [pair[1].split()]\n",
    "        bleu_score = sentence_bleu(real_words, output_words, weights = (0.5,0.5,0,0))\n",
    "        total_bleu += bleu_score\n",
    "    print(f\"mean BLEU score : {total_bleu / len(pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "233f13f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean BLEU score : 0.6727973615482881\n"
     ]
    }
   ],
   "source": [
    "only_decoder_encoder_BLEU = evaluate_BLEU(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4d2f7fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je ne suis pas medecin mais professeur .\n",
      "= i am not a doctor but a teacher .\n",
      "< i m not a doctor but teacher . <EOS>\n",
      "\n",
      "> nous sommes censes etre ensemble .\n",
      "= we re meant to be together .\n",
      "< we re meant to be together . <EOS>\n",
      "\n",
      "> il est sur le point de partir .\n",
      "= he s about to go .\n",
      "< he s about to leave . <EOS>\n",
      "\n",
      "> je ne suis pas plus intelligent que lui .\n",
      "= i m not as intelligent as he is .\n",
      "< i m not as smart as he . <EOS>\n",
      "\n",
      "> j agis pour mon pere .\n",
      "= i am acting for my father .\n",
      "< i am acting for my father . <EOS>\n",
      "\n",
      "> tu preches un convaincu .\n",
      "= you re preaching to the choir .\n",
      "< you re preaching to the choir . <EOS>\n",
      "\n",
      "> tu es tres genereuse .\n",
      "= you re very generous .\n",
      "< you re very generous . <EOS>\n",
      "\n",
      "> elle n est pas d humeur .\n",
      "= she s not in the mood .\n",
      "< she s not in the mood . <EOS>\n",
      "\n",
      "> j ai faim et soif .\n",
      "= i m hungry and thirsty .\n",
      "< i m thirsty and thirsty . <EOS>\n",
      "\n",
      "> tu es tres brave .\n",
      "= you re very brave .\n",
      "< you re very brave . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73921e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
